trainer: adsorbdiff.trainers.sde_denoising_trainer.DenoisingTrainer

dataset:
  - src: /home/jovyan/repos/ocp-modeling/data/is2re/all/train
  #- src: /home/jovyan/shared-scratch/adeesh/data/oc20_dense/lmdbs_train_subsplits/train
  #- src: /home/jovyan/shared-scratch/adeesh/data/oc20_dense/lmdbs/train_oc20dense_rs_overfit
    normalize_labels: False
  - src: /home/jovyan/repos/ocp-modeling/data/is2re/all/val_id/data.lmdb

logger:
  name: wandb
  project: ads_COM_diff

task:
  dataset: lmdb
  train_on_free_atoms: True
  eval_on_free_atoms: True
  relaxation_steps: 300
  relaxation_fmax: 0.01
  relax_dataset: 
        src: /home/jovyan/shared-scratch/adeesh/data/oc20_dense/lmdbs/rand1_I0.1
        #src: /home/jovyan/shared-scratch/adeesh/data/oc20_dense/lmdbs_corr/train_oc20dense_rs
        #src: /home/jovyan/shared-scratch/adeesh/denoising/com_denoising/com-xy_std0.01-10_numstep50x10_lr1.e-4_allcorr/final_struct_lmdb
        #src: /home/jovyan/shared-scratch/adeesh/data/oc20_dense/lmdbs_train_subsplits/val
  relax_opt:
        maxstep: 0.04
        memory: 50
        damping: 1.0
        alpha: 70.0
        #traj_dir: /home/jovyan/shared-scratch/adeesh/denoising/com_sde/bysigma_lmdbcorr_std0.1-10_numstep100_sample1
        traj_dir: /home/jovyan/shared-scratch/adeesh/denoising/overfit_sde/rand1I0.1_std0.1-10_numstep100_seed0
  primary_metric: loss
hide_eval_progressbar: False

model:
  name: adsorbdiff.models.painn.painn_denoising.PaiNN
  hidden_channels: 512
  num_layers: 6
  num_rbf: 128
  cutoff: 12.0
  max_neighbors: 50
  scale_file: configs/scaling_factors/painn_nb6_scaling_factors.pt
  regress_forces: True
  direct_forces: True
  use_pbc: True
  so3_denoising: True

optim:
  batch_size:                   48        # 6
  eval_batch_size:              48         # 6
  grad_accumulation_steps:      1         # gradient accumulation: effective batch size = `grad_accumulation_steps` * `batch_size` * (num of GPUs)
  load_balancing: atoms
  num_workers: 4
  lr_initial:                   1.e-4     # [0.0002, 0.0004], eSCN uses 0.0008 for batch size 96

  optimizer: AdamW
  optimizer_params:
    weight_decay: 0.001
  scheduler: LambdaLR
  scheduler_params:
    lambda_type: cosine
    warmup_factor: 0.2 #0.2
    warmup_epochs: 4 #0.5
    lr_min_factor: 0.01         #

  max_epochs: 100
  force_coefficient: 100
  energy_coefficient: 1
  clip_grad_norm: 100
  ema_decay: 0.999
  loss_energy: mae
  loss_force: l2mae

  # for denoising positions
  use_denoising_pos:            True
  denoising_pos_params:         
    prob:                       1      # probability to switch between denoising positions and S2EF
    fixed_noise_std:            False
    num_steps:                  100
    ads_std_low:                0.1      # lowest standard deviation of Gaussian noise for each xyz component
    ads_std_high:               10
    rot_std_low:               0.01
    rot_std_high:              1.55
    free_std_low:            0.01
    free_std_high:           0.1
    sample_low_forces:         True
    only_langevin:             True
  denoising_pos_coefficient:    1